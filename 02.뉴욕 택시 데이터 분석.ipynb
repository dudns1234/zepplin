{
  "metadata": {
    "name": "02.뉴욕 택시 데이터 분석",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n\r\nfrom pyspark.sql import SparkSession\r\nspark \u003d SparkSession.builder.appName(\"spark-dataframe\").getOrCreate()\r\nspark"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# 데이터가 들어있는 디렉토리 지정\ndirectory \u003d \"/home/ubuntu/working/datasource\"\n\ntrips_files \u003d \"trips/*\"\nzone_file \u003d \"taxi+_zone_lookup.csv\""
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntrips_df \u003d spark.read.csv(f\"file://{directory}/{trips_files}\", inferSchema\u003dTrue, header\u003dTrue)\nzone_df \u003d spark.read.csv(f\"file://{directory}/{zone_file}\", inferSchema\u003dTrue, header\u003dTrue)"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntrips_df.printSchema()\nzone_df.printSchema()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data Lake -\u003e  Data Warehouse\n\n- 필요한 데이터를 JOIN, 적절한 전처리, 필요한 데이터만 걸러내는 과정"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ntrips_df.createOrReplaceTempView(\"trips\")\nzone_df.createOrReplaceTempView(\"zone\")"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT * FROM trips;\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT * FROM zone;\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nquery \u003d \"\"\"\r\n    SELECT\r\n        t.VendorID as vendor_id,\r\n        TO_DATE(t.tpep_pickup_datetime) as pickup_date,\r\n        TO_DATE(t.tpep_dropoff_datetime) as dropoff_date,\r\n        HOUR(t.tpep_pickup_datetime) as pickup_time,\r\n        HOUR(t.tpep_dropoff_datetime) as dropoff_time,\r\n        \r\n        t.passenger_count,\r\n        t.trip_distance,\r\n        t.fare_amount,\r\n        t.tip_amount,\r\n        t.tolls_amount,\r\n        t.total_amount,\r\n        t.payment_type,\r\n        \r\n        pz.Zone as pickup_zone,\r\n        dz.Zone as dropoff_zone\r\n    FROM trips t\r\n    \r\n    LEFT JOIN zone pz ON t.PULocationID \u003d pz.locationID\r\n    LEFT JOIN zone dz ON t.DOLocationID \u003d dz.locationID\r\n\"\"\"\r\n\r\n#comb_df가 warehouse의 역할\r\ncomb_df \u003d spark.sql(query)\r\nz.show(comb_df)"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ncomb_df.createOrReplaceTempView(\"comb\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data Warehouse -\u003e Mart\n- 웨어하우스에서 마트를 만들기 위해서는 데이터를 검사, 정제\n- 마트에는 쓰레기 같은 데이터가 있으면 안됨."
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n-- 1. 날짜와 시간을 검사\nSELECT pickup_date, pickup_time\nFROM comb\nWHERE pickup_time \u003e\u003d 0\nORDER BY pickup_date"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT count(*)\nFROM comb\nWHERE pickup_date \u003c \u00272021-01-01\u0027"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 2. 요금 데이터 확인. 통계 정보를 확인\ncomb_df_describe \u003d comb_df.select(\"total_amount\").describe()\nz.show(comb_df_describe)\n\n# 결과\n# 요금이 - ??, 398469 \u003d 4억 ?? 요금이 마이너스고 4억이 나올 수는 없음. \u003d\u003e outlier 제거 필요"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# 3. 거리 데이터 확인\ncomb_df_distance \u003d comb_df.select(\"trip_distance\").describe()\nz.show(comb_df_distance)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n-- 4. 월별 운행수 확인\nSELECT DATE_TRUNC(\"MM\", pickup_date) as month, COUNT(*) as trips\nFROM comb\nGROUP BY month\nORDER BY month DESC"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# 5. 승객에 대한 통계정보 확인\ncomb_df_passenger_cnt \u003d comb_df.select(\"passenger_count\").describe()\nz.show(comb_df_passenger_cnt)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 살펴본 내용을 토대로 실제 분석할 데이터로 정제. Warehouse -\u003e Mart\n"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# 데이터 정제\nquery \u003d \"\"\"\nSELECT *\nFROM comb c\nWHERE c.total_amount \u003c 200 \n    AND c.total_amount \u003e 0 \n    \n    AND c.passenger_count \u003c 5 \n    \n    AND c.pickup_date \u003e\u003d \u00272021-01-01\u0027 \n    AND c.pickup_date \u003c \u00272021-08-01\u0027 \n    \n    AND c.trip_distance \u003c 10 \n    AND c.trip_distance \u003e 0\n\"\"\"\n\n# cleaned_df가 mart의 역할을 한다. 즉 데이터를 분석 할 상태가 되었음!\ncleaned_df \u003d spark.sql(query)\ncleaned_df.createOrReplaceTempView(\"cleaned\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nz.show(cleaned_df.describe())"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "EDA\n"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT pickup_date, COUNT(pickup_date) as trips\nFROM cleaned\nGROUP BY pickup_date\nORDER BY pickup_date ASC\n"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n-- 요일별 운행 수 확인\nSELECT \n    DATE_TRUNC(\"MM\", pickup_date) as month,\n    DATE_fORMAT(pickup_date,\u0027EEEE\u0027) as day_of_week,\n    COUNT(*) as trips\nFROM cleaned\nGROUP BY month, day_of_week"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 결제 유형 분석\nz.show(cleaned_df.select(\"payment_type\"))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# 숫자로 되어 있는 결제 타입을 문자열로 바꿔주기\ndef parse_payment_type(payment_type):\n\n    payment_type_to_string \u003d {\n      1: \"Credit Card\",\n      2: \"Cash\",\n      3: \"No Charge\",\n      4: \"Dispute\",\n      5: \"Unknown\",\n      6: \"Voided Trip\",\n    }\n\n    return payment_type_to_string[payment_type]"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# UDF 등록\nspark.udf.register(\"parse_payment_type\", parse_payment_type)"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\n-- 결제 타입별 통계\nSELECT \n    parse_payment_type(payment_type) as payment_type_str,\n    COUNT(*) as trips,\n    MEAN(fare_amount) as mean_fare_amount,\n    STD(fare_amount) as std_fare_amount\nFROM cleaned\nGROUP BY payment_type"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": "%%sql\n"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nspark.stop()\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}